{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LFGBE-XV3nd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import os\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_test, y_test), _ = cifar10.load_data()\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "p-btzInlWyND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_model(model_path, x_test, y_test, output_name, batch_size=32):\n",
        "    # Load model\n",
        "    model = tf.saved_model.load(model_path)\n",
        "    infer = model.signatures[\"serving_default\"]\n",
        "\n",
        "    # Benchmark latency and throughput\n",
        "    num_batches = len(x_test) // batch_size\n",
        "    latencies = []\n",
        "    total_samples = 0\n",
        "    start_total_time = time.time()\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_x = x_test[i * batch_size : (i + 1) * batch_size]\n",
        "        start_time = time.time()\n",
        "        _ = infer(tf.constant(batch_x))\n",
        "        latencies.append(time.time() - start_time)\n",
        "        total_samples += len(batch_x)\n",
        "\n",
        "    end_total_time = time.time()\n",
        "    avg_latency = np.mean(latencies)\n",
        "    throughput = total_samples / (end_total_time - start_total_time)\n",
        "\n",
        "    # Evaluate model accuracy\n",
        "    predictions = []\n",
        "    for i in range(num_batches):\n",
        "        batch_x = x_test[i * batch_size : (i + 1) * batch_size]\n",
        "        outputs = infer(tf.constant(batch_x))\n",
        "        predictions.extend(np.argmax(outputs[output_name].numpy(), axis=1))\n",
        "\n",
        "    accuracy = np.mean(np.argmax(y_test[:num_batches * batch_size], axis=1) == predictions)\n",
        "    return avg_latency, throughput, accuracy"
      ],
      "metadata": {
        "id": "riyKF40aXzS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model_path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(model_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # Convert bytes to MB"
      ],
      "metadata": {
        "id": "soImW7ieX3IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_memory():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,nounits,noheader\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        memory_used = int(result.stdout.splitlines()[0])  # Assume single GPU\n",
        "        return memory_used\n",
        "    except FileNotFoundError:\n",
        "        print(\"nvidia-smi not found. Skipping GPU memory measurement.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "kxkZ5m5pX5Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model_path = \"/content/drive/MyDrive/data_aug_model\"\n",
        "tensorrt_model_path = \"/content/drive/MyDrive/optimized_model\"\n",
        "keras_output = 'dense_3'\n",
        "tensor_rt_output = 'output_0'"
      ],
      "metadata": {
        "id": "Zr0qnU-wX8x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_latency, keras_throughput, keras_accuracy = benchmark_model(keras_model_path, x_test, y_test, keras_output)\n",
        "keras_size = get_model_size(keras_model_path)\n",
        "keras_memory = get_gpu_memory()\n",
        "print(f\"Keras Model - Latency: {keras_latency:.4f}s, Throughput: {keras_throughput:.2f} samples/s, \"\n",
        "      f\"Accuracy: {keras_accuracy:.4%}, Size: {keras_size:.2f} MB, GPU Memory: {keras_memory} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1eJZWdnYGRk",
        "outputId": "2306007f-e495-40c2-9bc4-26951c9b8f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras Model - Latency: 0.0081s, Throughput: 3927.45 samples/s, Accuracy: 91.3852%, Size: 32.57 MB, GPU Memory: 635 MB\n"
          ]
        }
      ]
    }
  ]
}